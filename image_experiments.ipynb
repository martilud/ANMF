{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data import *\n",
    "from utils import *\n",
    "from NMF import *\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling of data is done in data.py, and tensorflow/keras to load data. As long as these packages are installed, the experiments here should be replicated by any computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence experiment\n",
    "---\n",
    "\n",
    "We generate data, $2500$ of all types of data, with $0$ and $1$ digits. We select deterministic weights equal to $1/2$, and preselect $d = 32$. We do all experiments with the same data. The loss we show is the respective loss of the different methods. For all methods, we only show convergence of the first source. For ANMF we select the batch size of the adversarial data to be $100$ for all experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "mnist = MNIST()\n",
    "Ms = [0,1]\n",
    "M = len(Ms)\n",
    "N_sup = 2500\n",
    "N_sup_test = 0\n",
    "\n",
    "mnist.generate_supervised(Ms = Ms, N_sup = N_sup, N_sup_test = N_sup_test, type = \"det\", weights = [1.0/M] * M)\n",
    "\n",
    "V_sup = np.copy(mnist.x_sup_train[:,0,:,:].reshape((mnist.N_sup, 784)).T)\n",
    "U_s = np.copy(mnist.y_sup_train.reshape((mnist.N_sup,M,784)).T)\n",
    "U_sup = []\n",
    "U_test = np.copy(mnist.y_sup_test.reshape((mnist.N_sup_test,M,784)).T)\n",
    "U_test_fit = []\n",
    "for i in range(M):\n",
    "    U_sup.append(U_s[:,i,:])\n",
    "    U_test_fit.append(U_test[:,i,:])\n",
    "V_test = np.copy(mnist.x_sup_test[:,0,:,:].reshape((mnist.N_sup_test, 784)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence of NMF \n",
    "np.random.seed(0)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "batch_size = [50,500,1000,2500]\n",
    "linestyles = ['dashdot']*10\n",
    "times = []\n",
    "loss_stds = []\n",
    "for i,b in enumerate(batch_size):\n",
    "    tick = time.time()\n",
    "    nmf = NMF(d = 64, epochs = 100, batch_size = b, mu_W = 1e-10, mu_H = 1e-10, normalize = True)\n",
    "    loss_stds.append(nmf.fit_std(U_r = U_sup[0],conv = True))\n",
    "    tock = time.time()\n",
    "    times.append(tock-tick)\n",
    "print(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "for i,b in enumerate(batch_size): \n",
    "    plt.plot(np.arange(1,101), loss_stds[i][1:],label = f\"{b}\", linestyle = linestyles[i])\n",
    "plt.legend(title = \"Batch Size\", loc = \"upper right\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(f\"Convergence of SMU for NMF\")\n",
    "plt.grid()\n",
    "plt.savefig(\"fig/std_conv.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence of Adversarial NMF\n",
    "plt.rcParams.update({'font.size': 14}) \n",
    "np.random.seed(0)\n",
    "\n",
    "batch_size = [500,500,1000,2500]\n",
    "batch_size_z = [100,250,500,5000]\n",
    "linestyles = ['dashdot']*10\n",
    "times = []\n",
    "loss_advs = []\n",
    "\n",
    "for i,b in enumerate(batch_size):\n",
    "    tick = time.time()\n",
    "    nmf_sep = NMF_separation(ds = [64,64], epochs = 100, prob = \"adv\", true_sample = \"std\", normalize = True, update_H = False,\n",
    "        mu_W = 1e-10, mu_H = 1e-10, batch_size = batch_size[i], batch_size_z = batch_size_z[i], tau_A = 0.1)\n",
    "    U_z = nmf_sep.create_adversarial(U_sup, V_sup)\n",
    "    loss_advs.append(nmf_sep.NMFs[0].fit_adv(U_r = U_sup[0], U_z = U_z[0], conv = True))\n",
    "\n",
    "    tock = time.time()\n",
    "    times.append(tock-tick)\n",
    "print(times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "for i, b in enumerate(batch_size): \n",
    "    plt.plot(np.arange(1,len(loss_advs[i])), loss_advs[i][1:],label = f\"({b}, {batch_size_z[i]})\", linestyle = linestyles[i])\n",
    "plt.legend(title = r\"Batch Sizes \" + \"\\n\" + \"(True, Adv)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Convergence of SMU for ANMF\")\n",
    "plt.grid()\n",
    "plt.savefig(\"fig/adv_conv.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence of discriminative NMF\n",
    "np.random.seed(1)\n",
    "plt.rcParams.update({'font.size': 14}) \n",
    "\n",
    "batch_size_sup = [50,500,1000,2500]\n",
    "linestyles = ['dashdot']*10\n",
    "ds = [64,64]\n",
    "times = []\n",
    "loss_sups = []\n",
    "for i,b in enumerate(batch_size_sup):\n",
    "    tick = time.time()\n",
    "    nmf = NMF(d = np.sum(ds), ds = ds, epochs = 100, mu_W = 1e-10, mu_H = 1e-10, warm_start_epochs = 10,batch_size_sup = b, normalize = True)\n",
    "    loss_sups.append(nmf.fit_sup(U_sup = U_sup, V_sup = V_sup, conv = True))\n",
    "    tock = time.time()\n",
    "    times.append(tock-tick)\n",
    "print(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "for i, b in enumerate(batch_size_sup):\n",
    "    plt.plot(np.arange(1,101),loss_sups[i][0,1:],label = f\"{b}\", linestyle = linestyles[i])\n",
    "plt.legend(title = \"Batch Size\", loc = \"upper right\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Convergence of SMU for DNMF\")\n",
    "plt.grid()\n",
    "plt.savefig(\"fig/sup_conv.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergence of D+ANMF\n",
    "plt.rcParams.update({'font.size': 14}) \n",
    "np.random.seed(0)\n",
    "\n",
    "batch_size_r = [500,100,1000,2500]\n",
    "batch_size_z = [100, 100, 250, 5000]\n",
    "batch_size_sup = [100, 500, 1000, 2500]\n",
    "linestyles = ['dashdot']*10\n",
    "times = []\n",
    "loss_fulls = []\n",
    "for i,b in enumerate(batch_size_r):\n",
    "    tick = time.time()\n",
    "    nmf_sep = NMF_separation(ds = [64,64], epochs = 100, mu_W = 1e-10, mu_H = 1e-10, prob = \"full\",\n",
    "        batch_size = batch_size_r[i], batch_size_z = batch_size_z[i], batch_size_sup = batch_size_sup[i], tau_A = 0.05, tau_S = 0.5,\n",
    "        true_sample = \"sup\")\n",
    "    U_z = nmf_sep.create_adversarial(U_sup, V_sup)\n",
    "    loss_fulls.append(nmf_sep.NMF_concat.fit_full(U_r = U_sup, U_z = U_z, U_sup = U_sup, V_sup = V_sup, conv = True))\n",
    "    tock = time.time()\n",
    "    times.append(tock-tick)\n",
    "print(times)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "for i, b in enumerate(batch_size_r):\n",
    "    plt.plot(np.arange(1,101),loss_fulls[i][0,1:],label = f\"({b}, {batch_size_z[i]}, {batch_size_sup[i]})\", linestyle = linestyles[i])\n",
    "plt.legend(title = \"Batch Sizes\" + '\\n' + \"(True, Adv, Sup)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Convergence of SMU for D+ANMF\")\n",
    "plt.grid()\n",
    "plt.savefig(\"fig/full_conv.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for all methods converge, and smaller batch sizes tend to yield faster convergence. Selecting the batch sizes too small can lead to slower convergence and too much randomness. It is also interesting to see that standard NMF converges to a smaller loss than DNMF, because DNMF. This is to be expected as DNMF has to fit the basises while also splitting mixed data, while standard NMF only has to learn bases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data rich experiment\n",
    "---\n",
    "\n",
    "In this experiment we want to test the proposed methods in a setting where we have a lot of strong supervised data to see how the data settings compare. We test this on $0$ and $1$ digits, as NMF performs relatively well in this situation. We select $5000$ data of each source and $1000$ test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for experiment\n",
    "np.random.seed(1)\n",
    "number_of_experiments = 1\n",
    "M = 2\n",
    "Ms_all = [0,1]\n",
    "N_sup = 5000\n",
    "N_sup_test = 1000\n",
    "mu_W = 1e-8\n",
    "mu_H = 1e-6\n",
    "\n",
    "epochs = 100\n",
    "test_epochs = 100\n",
    "batch_size = 500\n",
    "batch_size_sup = 500\n",
    "batch_size_z = 100\n",
    "wiener = True\n",
    "\n",
    "Ds = [16,32,48,64,80,96,112,128]\n",
    "\n",
    "probs = [\"std\", \"adv\",\"sup\",\"exem\"]\n",
    "\n",
    "df = pd.DataFrame(columns = [\"id\", \"d\", \"prob\", \"mean_psnrs\"])\n",
    "\n",
    "taus = [0.0,0.1,0.0,0.0]\n",
    "\n",
    "\n",
    "for ex in range(number_of_experiments):\n",
    "    Ms = np.random.choice(Ms_all, M, replace = False)\n",
    "\n",
    "    # Generate data\n",
    "    mnist = MNIST()\n",
    "\n",
    "    mnist.generate_supervised(Ms = Ms, N_sup = N_sup, N_sup_test = N_sup_test, pytorch = False, type = \"det\", weights = [1.0/M] * M)\n",
    "    V_sup = np.copy(mnist.x_sup_train[:,0,:,:].reshape((mnist.N_sup, 784)).T)\n",
    "    U_s = np.copy(mnist.y_sup_train.reshape((mnist.N_sup,M,784)).T)\n",
    "    U_sup = []\n",
    "    for i in range(M):\n",
    "        U_sup.append(U_s[:,i,:])\n",
    "\n",
    "    V_test = np.copy(mnist.x_sup_test[:,0,:,:].reshape((mnist.N_sup_test, 784)).T)\n",
    "    U_test = np.copy(mnist.y_sup_test.reshape((mnist.N_sup_test,M,784)).T)\n",
    "\n",
    "    # arrays to store results\n",
    "    # Each problem, each source, each data\n",
    "    psnrs = np.zeros((len(probs), M, len(Ds), N_sup_test)) \n",
    "  \n",
    "\n",
    "    seps = []\n",
    "    for j,d in enumerate(Ds):\n",
    "        for i, prob in enumerate(probs):\n",
    "            print(prob)\n",
    "\n",
    "            # Fit\n",
    "            sep = NMF_separation(ds = [d] * M, tau_A = taus[i], normalize = True, update_H = False, true_sample = \"std\",\n",
    "                epochs = epochs,prob = prob, use_adv_basis = False,\n",
    "                mu_W = mu_W, mu_H = mu_H, test_epochs = test_epochs, wiener = True,\n",
    "                batch_size = batch_size, batch_size_z = d, batch_size_sup = batch_size_sup)\n",
    "\n",
    "            # Use standard NMF as initial conditions\n",
    "            if prob == \"adv\" or prob == \"sup\":\n",
    "                for k in range(M):\n",
    "                    sep.NMFs[k].W = np.copy(stdWs[k])\n",
    "\n",
    "            if prob == \"std\" or prob == \"exem\" or prob == \"adv\": \n",
    "                sep.fit(U_r = U_sup, V = V_sup)\n",
    "            else:\n",
    "                sep.fit(U_sup = U_sup, V_sup = V_sup)\n",
    "\n",
    "            if prob == \"std\":\n",
    "                stdWs = []\n",
    "                for k in range(M):\n",
    "                    stdWs.append(sep.NMFs[k].W)\n",
    "\n",
    "\n",
    "            # Separate\n",
    "            out = sep.separate(V_test)\n",
    "\n",
    "            # Measure quality SHOULD USE eval member function of sep\n",
    "            psnrs[i,:,j,:] = PSNR(U_test,out)\n",
    "\n",
    "            df = pd.concat([df, pd.DataFrame({\"d\": [d]*N_sup_test, \"id\": np.arange(0,N_sup_test), \"prob\": [prob]*N_sup_test, \"mean_psnrs\": np.mean(psnrs[i,:,j,:],axis = 0)})])\n",
    "            seps.append(deepcopy(sep))\n",
    "\n",
    "df.to_csv('data_rich.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "data_rich_df = pd.read_csv('data_rich.csv', index_col = 0)\n",
    "plt.rcParams.update({'font.size': 10}) \n",
    "seaborn.lineplot(data_rich_df, x = \"d\", y = \"mean_psnrs\", hue = \"prob\", \n",
    "    estimator = \"median\", errorbar = \"se\", markers = True, dashes = True ,\n",
    "    marker = 'o',legend=False)\n",
    "plt.grid()\n",
    "plt.ylabel(\"Median PSNR\")\n",
    "plt.legend(labels = [\"NMF\", \"_nolegend_\" , \"ANMF\", \"_nolegend_\", \"DNMF\", \"_nolegend_\", \"ENMF\"])\n",
    "plt.title(\"Impact of number of basis vectors for strong supervised data\")\n",
    "plt.savefig(\"fig/Data_rich.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that ANMF outperforms all other methods, and performance increases with $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data rich - Specific example\n",
    "---\n",
    "\n",
    "We now plot a specific data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick some separations\n",
    "seps_ = seps[12:16]\n",
    "for sep in seps_:\n",
    "    sep.wiener = True\n",
    "    print(sep.ds)\n",
    "\n",
    "psnrs_std = seps_[0].eval(U_test[:,:,:], V_test, \"psnr\" , axis = 0)\n",
    "psnrs_adv = seps_[1].eval(U_test[:,:,:], V_test, \"psnr\" , axis = 0)\n",
    "psnrs_sup = seps_[2].eval(U_test[:,:,:], V_test, \"psnr\" , axis = 0)\n",
    "psnrs_exem = seps_[3].eval(U_test[:,:,:], V_test, \"psnr\", axis = 0)\n",
    "\n",
    "out_std = seps_[0].separate(V_test[:,:])\n",
    "out_adv = seps_[1].separate(V_test[:,:])\n",
    "out_sup = seps_[2].separate(V_test[:,:])\n",
    "out_exem = seps_[3].separate(V_test[:,:])\n",
    "\n",
    "diff_adv = []\n",
    "\n",
    "for i in range(len(psnrs_std)):\n",
    "    diff_adv.append(psnrs_adv[i]-psnrs_std[i])\n",
    "\n",
    "ids = np.argsort(diff_adv)\n",
    "\n",
    "psnrs_std = seps_[0].eval(U_test[:,:,:], V_test[:,:], \"psnr\", aggregate = None)\n",
    "psnrs_adv = seps_[1].eval(U_test[:,:,:], V_test[:,:], \"psnr\", aggregate = None)\n",
    "psnrs_sup = seps_[2].eval(U_test[:,:,:], V_test[:,:], \"psnr\", aggregate = None)\n",
    "psnrs_exem = seps_[3].eval(U_test[:,:,:], V_test[:,:], \"psnr\", aggregate = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select id where adversarial outperforms standard NMF.\n",
    "id = ids[-1] \n",
    "vmin = 0\n",
    "vmax = 0.5\n",
    "\n",
    "plt.imshow(V_test[:,id].reshape((28,28)), cmap = \"gray\", vmin = vmin)\n",
    "plt.axis('off')\n",
    "plt.savefig('fig/v.png', bbox_inches = \"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.imshow(U_test[:,0,id].reshape((28,28)), cmap = \"gray\", vmin = vmin)\n",
    "plt.axis('off')\n",
    "plt.savefig('fig/u0.png', bbox_inches = \"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.imshow(U_test[:,1,id].reshape((28,28)), cmap = \"gray\", vmin = vmin)\n",
    "plt.axis('off')\n",
    "plt.savefig('fig/u1.png', bbox_inches = \"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(out_std[:,0,id].reshape((28,28)), cmap = \"gray\", vmin = vmin)\n",
    "ax.axis('off')\n",
    "plt.savefig('fig/u1_std.png')\n",
    "ax.text(0.02, 0.95, \"PSNR: {:.2f}\".format(psnrs_std[0,id]),\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        transform=ax.transAxes,\n",
    "        color='white', fontsize = 20)\n",
    "plt.savefig('fig/u1_std_psnr.png', bbox_inches = \"tight\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(out_std[:,1,id].reshape((28,28)), cmap = \"gray\", vmin = vmin)\n",
    "ax.axis('off')\n",
    "plt.savefig('fig/u0_std.png')\n",
    "ax.text(0.02, 0.95, \"PSNR: {:.2f}\".format(psnrs_std[1,id]),\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        transform=ax.transAxes,\n",
    "        color='white', fontsize = 20)\n",
    "plt.savefig('fig/u0_std_psnr.png', bbox_inches = \"tight\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(out_adv[:,0,id].reshape((28,28)), cmap = \"gray\", vmin = vmin)\n",
    "ax.axis('off')\n",
    "plt.savefig('fig/u1_adv.png')\n",
    "ax.text(0.02, 0.95, \"PSNR: {:.2f}\".format(psnrs_adv[0,id]),\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        transform=ax.transAxes,\n",
    "        color='white', fontsize = 20)\n",
    "plt.savefig('fig/u1_adv_psnr.png', bbox_inches = \"tight\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(out_adv[:,1,id].reshape((28,28)), cmap = \"gray\", vmin = vmin)\n",
    "ax.axis('off')\n",
    "plt.savefig('fig/u0_adv.png')\n",
    "ax.text(0.02, 0.95, \"PSNR: {:.2f}\".format(psnrs_adv[1,id]),\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        transform=ax.transAxes,\n",
    "        color='white', fontsize = 20)\n",
    "plt.savefig('fig/u0_adv_psnr.png', bbox_inches = \"tight\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(out_sup[:,0,id].reshape((28,28)), cmap = \"gray\", vmin = vmin)\n",
    "ax.axis('off')\n",
    "plt.savefig('fig/u1_sup.png')\n",
    "ax.text(0.02, 0.95, \"PSNR: {:.2f}\".format(psnrs_sup[0,id]),\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        transform=ax.transAxes,\n",
    "        color='white', fontsize = 20)\n",
    "plt.savefig('fig/u1_sup_psnr.png', bbox_inches = \"tight\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(out_sup[:,1,id].reshape((28,28)), cmap = \"gray\", vmin = vmin)\n",
    "ax.axis('off')\n",
    "plt.savefig('fig/u0_sup.png')\n",
    "ax.text(0.02, 0.95, \"PSNR: {:.2f}\".format(psnrs_sup[1,id]),\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        transform=ax.transAxes,\n",
    "        color='white', fontsize = 20)\n",
    "plt.savefig('fig/u0_sup_psnr.png', bbox_inches = \"tight\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(out_exem[:,0,id].reshape((28,28)), cmap = \"gray\", vmin = vmin)\n",
    "ax.axis('off')\n",
    "plt.savefig('fig/u1_exem.png')\n",
    "ax.text(0.02, 0.95, \"PSNR: {:.2f}\".format(psnrs_exem[0,id]),\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        transform=ax.transAxes,\n",
    "        color='white', fontsize = 20)\n",
    "plt.savefig('fig/u1_exem_psnr.png', bbox_inches = \"tight\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(out_exem[:,1,id].reshape((28,28)), cmap = \"gray\", vmin = vmin)\n",
    "ax.axis('off')\n",
    "plt.savefig('fig/u1_exem.png')\n",
    "ax.text(0.02, 0.95, \"PSNR: {:.2f}\".format(psnrs_exem[1,id]),\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        transform=ax.transAxes,\n",
    "        color='white', fontsize = 20)\n",
    "plt.savefig('fig/u0_exem_psnr.png', bbox_inches = \"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data rich - Effect of $\\tau_A$\n",
    "---\n",
    "\n",
    "We now run an equivalent experiment where we try several values of $\\tau_A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "number_of_experiments = 1\n",
    "M = 2\n",
    "Ms_all = [0,1]\n",
    "N_adv = 1000\n",
    "N_sup = 5000\n",
    "N_sup_test = 1000\n",
    "mu_W = 1e-8\n",
    "mu_H = 1e-6\n",
    "\n",
    "epochs_std = 100\n",
    "epochs = 100\n",
    "test_epochs = 100\n",
    "batch_size = 500\n",
    "batch_size_sup = 500\n",
    "batch_size_z = 100\n",
    "wiener = True\n",
    "\n",
    "Ds = [32,64,96,128]\n",
    "\n",
    "probs = [\"std\", \"adv\",\"adv\",\"adv\", \"adv\", \"adv\", \"adv\"]\n",
    "\n",
    "df = pd.DataFrame(columns = [\"id\", \"d\", \"prob\", \"tau\", \"mean_psnrs\"])\n",
    "\n",
    "taus = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5]\n",
    "\n",
    "\n",
    "for ex in range(number_of_experiments):\n",
    "    Ms = np.random.choice(Ms_all, M, replace = False)\n",
    "\n",
    "    # Generate data\n",
    "    mnist = MNIST()\n",
    "    #mnist.generate_adverserial(Ms = Ms, Ns = [N_adv]*M, N_V = N_V, type = \"det\", weights = [1.0/M]*M)\n",
    "    #U_r = []\n",
    "    #for i in range(M):\n",
    "    #    U_r.append(np.copy(mnist.x_r_train[i].reshape((mnist.Ns_adv[i],784)).T))\n",
    "    #V = np.copy(mnist.x_v_train.reshape((mnist.N_adv_V,784)).T)\n",
    "\n",
    "    mnist.generate_supervised(Ms = Ms, N_sup = N_sup, N_sup_test = N_sup_test, pytorch = False, type = \"det\", weights = [1.0/M] * M)\n",
    "    V_sup = np.copy(mnist.x_sup_train[:,0,:,:].reshape((mnist.N_sup, 784)).T)\n",
    "    U_s = np.copy(mnist.y_sup_train.reshape((mnist.N_sup,M,784)).T)\n",
    "    U_sup = []\n",
    "    for i in range(M):\n",
    "        U_sup.append(U_s[:,i,:])\n",
    "\n",
    "    V_test = np.copy(mnist.x_sup_test[:,0,:,:].reshape((mnist.N_sup_test, 784)).T)\n",
    "    U_test = np.copy(mnist.y_sup_test.reshape((mnist.N_sup_test,M,784)).T)\n",
    "\n",
    "    # arrays to store results\n",
    "    psnrs = np.zeros((len(probs), M, len(Ds), N_sup_test)) \n",
    "\n",
    "    seps = []\n",
    "    for j,d in enumerate(Ds):\n",
    "        for i, prob in enumerate(probs):\n",
    "            print(prob)\n",
    "\n",
    "            # Fit\n",
    "            sep = NMF_separation(ds = [d] * M, tau_A = taus[i], normalize = True, update_H = False, true_sample = \"std\",\n",
    "                epochs = epochs_std if prob == \"std\" else epochs,prob = prob, use_adv_basis = False,\n",
    "                mu_W = mu_W, mu_H = mu_H, test_epochs = test_epochs, wiener = True,\n",
    "                batch_size = batch_size, batch_size_z = d, batch_size_sup = batch_size_sup)\n",
    "\n",
    "            # Use standard NMF as initial conditions\n",
    "            if prob == \"adv\" or prob == \"sup\":\n",
    "                for k in range(M):\n",
    "                    sep.NMFs[k].W = np.copy(stdWs[k])\n",
    "            \n",
    "            sep.fit(U_r = U_sup, V = V_sup, U_sup = U_sup, V_sup = V_sup)\n",
    "\n",
    "            if prob == \"std\":\n",
    "                stdWs = []\n",
    "                for k in range(M):\n",
    "                    stdWs.append(sep.NMFs[k].W)\n",
    "\n",
    "\n",
    "            # Separate\n",
    "            out = sep.separate(V_test)\n",
    "\n",
    "            # Measure quality SHOULD USE eval member function of sep\n",
    "            psnrs[i,:,j,:] = PSNR(U_test,out)\n",
    "\n",
    "            df = pd.concat([df, pd.DataFrame({\"d\": [d]*N_sup_test, \"id\": np.arange(0,N_sup_test), \"prob\": [prob]*N_sup_test, \"tau\": [taus[i]]*N_sup_test, \"mean_psnrs\": np.mean(psnrs[i,:,j,:],axis = 0)})])\n",
    "            seps.append(deepcopy(sep))\n",
    "\n",
    "df.to_csv('data_rich_tau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "data_rich_tau_df = pd.read_csv('data_rich_tau.csv', index_col = 0)\n",
    "plt.rcParams.update({'font.size': 10}) \n",
    "seaborn.lineplot(data_rich_tau_df, x = \"tau\", y = \"mean_psnrs\", hue = \"d\",\n",
    "    estimator = \"median\", errorbar = \"se\", markers = True, dashes = True,\n",
    "    marker = 'o',legend=False, palette = \"flare\")\n",
    "plt.grid()\n",
    "plt.ylabel(\"Median PSNR\")\n",
    "plt.legend(labels = [r\"$d = 32$\", \"_nolegend_\" , r\"$d = 64$\", \"_nolegend_\", r\"$d = 96$\", \"_nolegend\", r\"$d = 128$\"])\n",
    "plt.title(\"Impact of \" + r\"$\\tau_A$\" + \" for ANMF\")\n",
    "plt.xlabel(r\"$\\tau_A$\")\n",
    "plt.savefig(\"fig/Data_rich_tau.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data poor tuning experiment\n",
    "---\n",
    "\n",
    "We now do a larger tuning experiment where we have low amounts of data. We select $250$ strong supervised data and $500$ weak supervised data of each source. We mix each digit with a \"one\"-digit and use random search to find the best parameters for this specific setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BIG NUMERICAL EXAMPLE WITH RANDOM SEARCH\n",
    "\"\"\"\n",
    "np.random.seed(0)\n",
    "number_of_experiments = 9\n",
    "number_of_searches = 10\n",
    "M = 2\n",
    "Ms_all = [0,2,3,4,5,6,7,8,9]\n",
    "N_adv = 500\n",
    "N_V = 0\n",
    "N_sup = 250\n",
    "N_sup_test = 1000\n",
    "\n",
    "d_dict = {\"name\": 'ds', \"dist\": lambda: [64, 64]}\n",
    "normalize_dict = {\"name\": 'normalize', \"dist\": lambda: np.random.choice([True,False], replace = True)}\n",
    "mu_H_dict = {\"name\": 'mu_H', \"dist\": lambda: np.power(10,np.random.uniform(-10,-3))}\n",
    "mu_W_dict = {\"name\": 'mu_W', \"dist\": lambda: np.power(10,np.random.uniform(-10,-3))}\n",
    "epochs_dict = {\"name\": 'epochs', \"dist\": lambda: np.random.randint(1,100)}\n",
    "test_epochs_dict = {\"name\": 'test_epochs', \"dist\": lambda: np.random.randint(100,150)}\n",
    "batch_size_r_dict = {\"name\": 'batch_size', \"dist\": lambda: np.random.choice([250,500], replace = True)}\n",
    "batch_size_z_dict = {\"name\": 'batch_size_z', \"dist\": lambda: np.random.choice([125,250], replace = True)}\n",
    "batch_size_sup_dict = {\"name\": 'batch_size_sup', \"dist\": lambda: np.random.choice([125], replace = True)}\n",
    "true_sample_dict = {\"name\": 'true_sample', \"dist\": lambda: np.random.choice([\"std\", \"sup\"], replace = True)}\n",
    "tau_A_dict = {\"name\": 'tau_A', \"dist\": lambda: np.random.uniform(0.0,0.3)}\n",
    "tau_S_dict = {\"name\": 'tau_S', \"dist\": lambda: np.random.uniform(0.0,0.5)}\n",
    "betas_dict = {\"name\": 'betas', \"dist\": lambda: [np.random.uniform(0.5,1.0), np.random.uniform(0.5,1.0)]} \n",
    "\n",
    "probs = [\"exem\", \"std\", \"adv\", \"sup\", \"full\"]\n",
    "results = {\"M\": [], \"exem\" : [], \"std\" : [], \"adv\" : [], \"sup\" : [], \"full\": []}\n",
    "\n",
    "mnist = MNIST()\n",
    "\n",
    "for i in range(number_of_experiments):\n",
    "\n",
    "\n",
    "    Ms = [1,i + (i>=1)]\n",
    "    results[\"M\"].append(i + (i>=1))\n",
    "\n",
    "    mnist.generate_adverserial(Ms = Ms, Ns = [N_adv]*M, N_V = N_V, type = \"det\", weights = [1.0/M]*M)\n",
    "    U_r = []\n",
    "    for i in range(M):\n",
    "        U_r.append(np.copy(mnist.x_r_train[i][:,:,:].reshape((mnist.Ns_adv[i],784)).T))\n",
    "    V = np.copy(mnist.x_v_train.reshape((mnist.N_adv_V,784)).T)\n",
    "    \n",
    "    mnist.generate_supervised(Ms = Ms, N_sup = N_sup, N_sup_test = N_sup_test, type = \"det\", weights = [1.0/M] * M)\n",
    "    \n",
    "    V_sup = np.copy(mnist.x_sup_train[:,0,:,:].reshape((mnist.N_sup, 784)).T)\n",
    "    U_s = np.copy(mnist.y_sup_train.reshape((mnist.N_sup,M,784)).T)\n",
    "    U_sup = []\n",
    "    U_test = np.copy(mnist.y_sup_test.reshape((mnist.N_sup_test,M,784)).T)\n",
    "    U_test_fit = []\n",
    "    for i in range(M):\n",
    "        U_sup.append(U_s[:,i,:])\n",
    "        U_test_fit.append(U_test[:,i,:])\n",
    "    V_test = np.copy(mnist.x_sup_test[:,0,:,:].reshape((mnist.N_sup_test, 784)).T)\n",
    "\n",
    "\n",
    "    for prob in probs:\n",
    "        print(prob)\n",
    "\n",
    "        prob_dict = {\"name\": 'prob', \"dist\": lambda: prob}\n",
    "\n",
    "        if prob == \"exem\" or prob == \"std\":\n",
    "            W_dict = {\"name\": 'Ws', \"dist\": lambda: None}\n",
    "\n",
    "        param_dicts = [d_dict,\n",
    "            normalize_dict,\n",
    "            mu_H_dict, \n",
    "            prob_dict, \n",
    "            mu_W_dict, \n",
    "            epochs_dict, \n",
    "            test_epochs_dict,\n",
    "            batch_size_r_dict, \n",
    "            batch_size_z_dict, \n",
    "            batch_size_sup_dict,\n",
    "            tau_A_dict,\n",
    "            tau_S_dict,\n",
    "            true_sample_dict,\n",
    "            betas_dict,\n",
    "            W_dict,\n",
    "        ]\n",
    "\n",
    "        rs = random_search(NMF_separation, param_dicts, N_ex = number_of_searches, cv = 0 if prob != \"full\" and prob != \"sup\" else 2, source_aggregate = \"mean\", data_aggregate = \"median\")\n",
    "        rs.fit(U_r = U_r, V = V, U_sup = U_sup, V_sup = V_sup, refit = True)\n",
    "\n",
    "        results[prob].append(np.median(rs.best_model.eval(U_test[:,:,:],V_test[:,:], \"psnr\")))\n",
    "\n",
    "        if prob == \"std\" or prob == \"exem\":\n",
    "            Ws = [np.copy(rs.best_model.NMFs[0].W), np.copy(rs.best_model.NMFs[1].W)]\n",
    "            W_dict = {\"name\": 'Ws', \"dist\": lambda : Ws}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)\n",
    "data_poor_df = pd.DataFrame(results)\n",
    "data_poor_df.to_csv('data_poor_tuning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_poor_df = pd.read_csv('data_poor_tuning.csv', index_col = 0)\n",
    "#for i in range(len(data_poor_df)):\n",
    "#    data_poor_df[\"M\"].loc[i] = i + (i>=1)\n",
    "new_df = data_poor_df.copy(deep = True)\n",
    "#\n",
    "for column in new_df:\n",
    "    if column != \"M\":\n",
    "        new_df[column] = new_df[column] - data_poor_df[\"std\"]\n",
    "ax = seaborn.lineplot(data = new_df.drop(\"M\", axis = 1).drop(\"exem\", axis = 1), marker = 'o')\n",
    "ax.set_xticklabels(['','zero', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'])\n",
    "ax.set_xlabel('Digit mixed with \"one\" digit')\n",
    "ax.set_ylabel(r\"$\\Delta$\" + \"Median PSNR\")\n",
    "plt.title(\"Comparison in the constrained data setting for different classes of digits\")\n",
    "plt.legend(labels = [\"NMF\", \"_nolegend_\", \"ANMF\", \"_nolegend_\", \"DNMF\", \"_nolegend_\", \"D+ANMF\"])\n",
    "plt.grid()\n",
    "plt.savefig('fig/data_poor.pdf')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
